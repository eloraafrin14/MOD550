import os
import subprocess
import numpy as np
import matplotlib.pyplot as plt
import json
import pandas as pd

# ===============================
# 1. Define the DatasetGenerator
# ===============================
class DatasetGenerator:
    def __init__(self, n_points=100):
        self.n_points = n_points
        self.dataset = None
        self.metadata = {}

    def generate_random(self, x_range=(0, 1), y_range=(0, 1)):
        """Generates a random 2D dataset."""
        xs = np.random.uniform(x_range[0], x_range[1], self.n_points)
        ys = np.random.uniform(y_range[0], y_range[1], self.n_points)
        self.dataset = np.column_stack((xs, ys))
        self.metadata = {
            "type": "random",
            "x_range": x_range,
            "y_range": y_range
        }
        return self.dataset

    def generate_noisy_function(self, func, x_range=(0, 10), noise_std=1.0):
        """
        Generates a 2D dataset with y-values computed as func(x) plus Gaussian noise.
        
        Parameters:
            func (callable): The function to generate y values.
            x_range (tuple): The range for x values.
            noise_std (float): Standard deviation of the noise.
        """
        xs = np.linspace(x_range[0], x_range[1], self.n_points)
        ys = func(xs) + np.random.normal(0, noise_std, self.n_points)
        self.dataset = np.column_stack((xs, ys))
        try:
            func_name = func.__name__
        except AttributeError:
            func_name = str(func)
        self.metadata = {
            "type": "noisy_function",
            "x_range": x_range,
            "noise_std": noise_std,
            "function": func_name
        }
        return self.dataset

    def generate_custom(self):
        """
        Generates a custom non-linear dataset.
        Here we generate a noisy sine wave (which is not a line).
        """
        xs = np.linspace(0, 2 * np.pi, self.n_points)
        ys = np.sin(xs) + np.random.normal(0, 0.1, self.n_points)
        self.dataset = np.column_stack((xs, ys))
        self.metadata = {
            "type": "custom",
            "function": "sine",
            "description": "Noisy sine wave"
        }
        return self.dataset

# =====================================
# A sample function for use in datasets
# =====================================
def quadratic(x):
    """A quadratic function used for simulating an imported dataset."""
    return 0.5 * x**2 - 3 * x + 2

# ===============================
# Utility Functions
# ===============================
def ensure_directory(dir_path):
    """Ensure that the directory exists; if not, create it."""
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

def save_dataset_to_csv(dataset, filename):
    """Save the 2D dataset to a CSV file."""
    np.savetxt(filename, dataset, delimiter=',', header='x,y', comments='')

def save_metadata(metadata, filename):
    """Save metadata (a dict) as a JSON file."""
    with open(filename, 'w') as f:
        json.dump(metadata, f, indent=4)

def load_dataset_from_csv(filename):
    """Load a 2D dataset from a CSV file."""
    return np.loadtxt(filename, delimiter=',', skiprows=1)

def load_metadata(filename):
    """Load metadata from a JSON file."""
    with open(filename, 'r') as f:
        return json.load(f)

def plot_data_with_model(dataset, model_func, model_label, output_file):
    """
    Plot the dataset and overlay a curve generated by model_func.
    
    Parameters:
        dataset (ndarray): 2D data points.
        model_func (callable): The assumed model function.
        model_label (str): Label for the model curve.
        output_file (str): Filename to save the plot.
    """
    plt.figure(figsize=(8, 6))
    plt.scatter(dataset[:, 0], dataset[:, 1], label='Imported Data', color='blue')
    x_min, x_max = min(dataset[:, 0]), max(dataset[:, 0])
    x_dense = np.linspace(x_min, x_max, 200)
    y_model = model_func(x_dense)
    plt.plot(x_dense, y_model, label=model_label, color='red')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Imported Data and Assumed Model')
    plt.legend()
    plt.grid(True)
    plt.savefig(output_file)
    plt.close()

def create_coding_assessment_text(output_file):
    """
    Create a text file with an assessment of three GitHub/GitLab repositories.
    """
    assessment_text = (
        "Coding Standards Assessment:\n\n"
        "Repository 1: Requests (https://github.com/psf/requests)\n"
        "- Adheres closely to PEP8 guidelines and includes comprehensive docstrings.\n"
        "- The codebase is modular and highly readable, making it an excellent example of good coding practices.\n\n"
        "Repository 2: Flask (https://github.com/pallets/flask)\n"
        "- Demonstrates a clear and consistent coding style with a well-organized codebase.\n"
        "- Its modular design, detailed documentation, and effective use of design patterns contribute to its maintainability.\n\n"
        "Repository 3: Django (https://github.com/django/django)\n"
        "- Enforces strict coding standards despite being a large framework.\n"
        "- Features comprehensive testing, thorough documentation; and a clear separation of concerns, making it a strong model for code quality and maintainability.\n"
    ) 
    with open(output_file, 'w') as f:
        f.write(assessment_text)


# ===================================================
# NEW: Function to import the Iris dataset from UIUC
# ===================================================
def import_iris_dataset():
    """
    Imports the Iris dataset from the UIUC data-fa14 repository,
    converts it into a 2D dataset using two selected columns,
    and returns the dataset along with metadata.
    """
    repo_url = "https://github.com/uiuc-cse/data-fa14.git"
    # Use the specified folder structure: MOD550/data/data-fa14
    clone_path = os.path.join("data", "imported_data")
    
    # Clone the repository if it does not exist
    if not os.path.exists(clone_path):
        try:
            subprocess.run(["git", "clone", repo_url, clone_path], check=True)
            print("Cloned the data-fa14 repository.")
        except subprocess.CalledProcessError as e:
            print("Error cloning the repository:", e)
            return None, None
    else:
        print("data-fa14 repository already exists.")
    
    # iris dataset path
    iris_csv_path = os.path.join("data", "imported_data", "data", "iris.csv")
    
    if not os.path.exists(iris_csv_path):
        raise FileNotFoundError(f"{iris_csv_path} not found. Please verify that the repository contains the Iris dataset.")
    
    # Read the CSV using pandas
    df = pd.read_csv(iris_csv_path)
    print("Columns in Iris dataset:", df.columns.tolist())
    
    # Attempt to extract 'sepal_length' and 'sepal_width'
    try:
        data_2d = df[['sepal_length', 'sepal_width']].to_numpy()
        columns_used = ['sepal_length', 'sepal_width']
    except KeyError:
        # Try alternative column names if needed
        try:
            data_2d = df[['SepalLength', 'SepalWidth']].to_numpy()
            columns_used = ['SepalLength', 'SepalWidth']
        except KeyError as err:
            raise KeyError("Expected columns not found in Iris dataset.") from err

    metadata = {
        "source": "Iris dataset from UIUC data-fa14 repository",
        "file": iris_csv_path,
        "columns_used": columns_used,
        "description": "2D dataset extracted from the Iris dataset by selecting two numerical features: sepal length and sepal width.",
        "truth": "No explicit model; data extracted from real measurements."
    }
    return data_2d, metadata

# ===============================
# Main Function
# ===============================
def main():
    # Update directories
    data_dir = os.path.join("data")
    ensure_directory(data_dir)
    
    # ------------------------------------------------------------
    # 1 & 2. Generate two datasets using the DatasetGenerator class.
    # ------------------------------------------------------------
    # Dataset 1: Random 2D points
    generator1 = DatasetGenerator(n_points=100)
    dataset_random = generator1.generate_random(x_range=(0, 10), y_range=(0, 10))
    metadata_random = generator1.metadata

    # Dataset 2: Noisy samples around a sine function
    generator2 = DatasetGenerator(n_points=100)
    dataset_noisy = generator2.generate_noisy_function(np.sin, x_range=(0, 2 * np.pi), noise_std=0.2)
    metadata_noisy = generator2.metadata

    # ------------------------------------------------------------
    # 3. Append one dataset to the other to form a combined dataset.
    # ------------------------------------------------------------
    combined_dataset = np.vstack((dataset_random, dataset_noisy))
    combined_metadata = {
        "dataset1": metadata_random,
        "dataset2": metadata_noisy
    }

    # ------------------------------------------------------------
    # 4. Write the combined dataset and its metadata to files.
    # ------------------------------------------------------------
    combined_dataset_file = os.path.join(data_dir, "combined_dataset.csv")
    combined_metadata_file = os.path.join(data_dir, "combined_metadata.json")
    save_dataset_to_csv(combined_dataset, combined_dataset_file)
    save_metadata(combined_metadata, combined_metadata_file)

    # ------------------------------------------------------------
    # 6. Import an external dataset (the Iris dataset).
    # ------------------------------------------------------------
    imported_dataset_file = os.path.join(data_dir, "imported_dataset.csv")
    imported_metadata_file = os.path.join(data_dir, "imported_metadata.json")
    
    # If files do not exist, import the dataset from the external repository
    if not os.path.exists(imported_dataset_file) or not os.path.exists(imported_metadata_file):
        imported_dataset, imported_metadata = import_iris_dataset()
        if imported_dataset is None:
            print("Failed to import the Iris dataset. Exiting.")
            return
        # Save the imported dataset and metadata
        save_dataset_to_csv(imported_dataset, imported_dataset_file)
        save_metadata(imported_metadata, imported_metadata_file)
    else:
        imported_dataset = load_dataset_from_csv(imported_dataset_file)
        imported_metadata = load_metadata(imported_metadata_file)

    # ------------------------------------------------------------
    # 7. Plot the imported dataset and overlay an assumed model.
    # For the Iris dataset, we can compute a simple linear regression as our assumed model.
    # ------------------------------------------------------------
    x = imported_dataset[:, 0]
    y = imported_dataset[:, 1]
    coeffs = np.polyfit(x, y, 1)  # Linear regression: y = m*x + b
    assumed_model_func = np.poly1d(coeffs)
    assumed_model_label = f"Linear Fit: y = {coeffs[0]:.2f}x + {coeffs[1]:.2f}"

    plot_output_file = os.path.join(data_dir, "imported_data_plot.png")
    plot_data_with_model(imported_dataset, assumed_model_func, assumed_model_label, plot_output_file)


    # ------------------------------------------------------------
    # 8. Create a text file with an assessment of 3 repositories.
    # ------------------------------------------------------------
    coding_assessment_file = os.path.join(data_dir, "coding_assessment.txt")
    create_coding_assessment_text(coding_assessment_file)

    # Inform the user of the output locations.
    print("Data generation and processing complete.\n")
    print("Combined dataset saved to:", combined_dataset_file)
    print("Combined metadata saved to:", combined_metadata_file)
    print("Imported data plot saved to:", plot_output_file)
    print("Coding assessment saved to:", coding_assessment_file)

if __name__ == "__main__":
    main()
